# RecurrentPPO agent configuration
# LSTM-based policy for POMDP environments with delayed/noisy observations
# Uses sb3-contrib's RecurrentPPO with MlpLstmPolicy

algorithm: "RecurrentPPO"

# LSTM-specific architecture parameters
lstm_kwargs:
  lstm_hidden_size: 64  # Hidden state dimension for LSTM layers
  n_lstm_layers: 1      # Number of stacked LSTM layers
  enable_critic_lstm: true  # Whether critic network also uses LSTM (recommended for POMDPs)

# Policy network architecture (pre-LSTM feature extraction)
policy_kwargs:
  net_arch:
    - 128
    - 128

# RecurrentPPO algorithm hyperparameters (same as standard PPO)
recurrent_ppo:
  learning_rate: 3.0e-4
  n_steps: 2048
  batch_size: 64
  n_epochs: 10
  gamma: 0.99
  gae_lambda: 0.95
  clip_range: 0.2
  ent_coef: 0.0
  vf_coef: 0.5
  max_grad_norm: 0.5
  verbose: 0

