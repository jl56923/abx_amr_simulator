# Base experiment configuration
# References component configs with minimal setup
# Override parameters via command-line flags

# REPRODUCIBILITY NOTE:
# The seed in the 'training' section below is the SINGLE SOURCE OF TRUTH for the entire experiment.
# It controls determinism across:
# - PatientGenerator (patient sampling)
# - RewardCalculator (deterministic reward computation)
# - Environment (initial state, patient generation)
# - Agent (policy initialization, exploration)
# Setting seed: <value> ensures fully reproducible training runs.

# Component configurations
environment: ../environment/default.yaml
reward_calculator: ../reward_calculator/default.yaml
patient_generator: ../patient_generator/default.yaml
agent_algorithm: ../agent_algorithm/default.yaml

# Training configuration
# Note: All frequencies are specified in episodes, not timesteps
# Conversion to timesteps happens internally: timesteps = episodes * environment.max_time_steps
training:
  run_name: example_run
  total_num_training_episodes: 25
  save_freq_every_n_episodes: 5
  eval_freq_every_n_episodes: 5
  num_eval_episodes: 10
  seed: 42  # Single source of truth for experiment reproducibility (see note above)
  log_patient_trajectories: true  # Set to true to save full patient trajectories during eval
  
  # Early stopping configuration (optional)
  # Set enabled: true to stop training when performance plateaus
  early_stopping:
    enabled: false  # Set to true to enable early stopping
    patience: 5     # Number of evaluations without improvement before stopping
    min_delta: 0.0  # Minimum improvement threshold (absolute, not percentage)
    metric_name: "eval/mean_reward"  # Metric to monitor from EvalCallback