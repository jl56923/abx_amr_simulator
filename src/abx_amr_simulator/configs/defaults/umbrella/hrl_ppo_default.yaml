# HRL PPO default experiment configuration
# Demonstrates required HRL fields for option-based training.

# Folder locations
# These specify where to find component configs and option libraries.
# Paths are relative to the umbrella config's directory.
config_folder_location: ../
options_folder_location: ../../options

# Component configurations
# Paths are relative to config_folder_location
environment: environment/default.yaml
reward_calculator: reward_calculator/default.yaml
patient_generator: patient_generator/default.yaml
agent_algorithm: agent_algorithm/hrl_ppo.yaml

# HRL-specific configuration
# Note: set up_options_folders_with_defaults() to create the options folder
# before training, then point option_library to the generated library YAML.
hrl:
  option_library: option_libraries/default_deterministic.yaml
# Training configuration
training:
  run_name: hrl_ppo_default
  total_num_training_episodes: 25
  save_freq_every_n_episodes: 5
  eval_freq_every_n_episodes: 5
  num_eval_episodes: 10
  seed: 42
  log_patient_trajectories: true

  # Early stopping configuration (optional)
  early_stopping:
    enabled: false
    patience: 5
    min_delta: 0.0
    metric_name: "eval/mean_reward"
