# HRL PPO default experiment configuration
# Demonstrates required HRL fields for option-based training.

# Component configurations
environment: ../environment/default.yaml
reward_calculator: ../reward_calculator/default.yaml
patient_generator: ../patient_generator/default.yaml
agent_algorithm: ../agent_algorithm/hrl_ppo.yaml

# HRL-specific configuration
# Note: run setup_options_folders_with_defaults() to create the options folder
# before training, then point option_library to the generated library YAML.
hrl:
  option_library: experiments/options/option_libraries/default_deterministic.yaml
  option_gamma: 0.99
  front_edge_use_full_vector: false

# Training configuration
training:
  run_name: hrl_ppo_default
  total_num_training_episodes: 25
  save_freq_every_n_episodes: 5
  eval_freq_every_n_episodes: 5
  num_eval_episodes: 10
  seed: 42
  log_patient_trajectories: true

  # Early stopping configuration (optional)
  early_stopping:
    enabled: false
    patience: 5
    min_delta: 0.0
    metric_name: "eval/mean_reward"
