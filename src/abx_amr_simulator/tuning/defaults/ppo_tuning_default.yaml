# Default PPO hyperparameter tuning configuration.
#
# This tuning config defines the search space and optimization settings for
# standard PPO agents (non-HRL). Use with tune.py to find optimal hyperparameters.
#
# Compatible algorithms: PPO, A2C
#
# Example usage:
#   python -m abx_amr_simulator.training.tune \
#     --umbrella-config /path/to/base_experiment.yaml \
#     --tuning-config /path/to/ppo_tuning_default.yaml \
#     --run-name ppo_tuning_baseline
#
# Search space covers the most impactful PPO hyperparameters:
# - learning_rate: Controls policy update step size
# - n_steps: Rollout length (affects sample efficiency vs variance)
# - gamma: Discount factor (affects long-term vs short-term rewards)
# - gae_lambda: Generalized Advantage Estimation parameter
# - ent_coef: Entropy coefficient (encourages exploration)
# - clip_range: PPO clipping parameter (limits policy change per update)

optimization:
  # Number of trials to run (each trial tests one hyperparameter combination)
  n_trials: 50
  
  # Number of random seeds per trial (results aggregated with mean-variance penalty)
  # Higher values increase robustness but increase runtime
  n_seeds_per_trial: 3
  
  # Reduced episode count for fast tuning trials
  # Should be long enough to show learning signal but shorter than full training
  truncated_episodes: 50
  
  # Optimization objective ('maximize' for reward, 'minimize' for loss)
  direction: maximize
  
  # Sampler algorithm: 'TPE' (Tree-structured Parzen Estimator), 'Random', or 'CMAES'
  # TPE is recommended for most cases (adaptive Bayesian optimization)
  sampler: TPE
  
  # Stability penalty weight λ for mean-variance objective: mean(rewards) - λ * std(rewards)
  # λ=0.0: Pure mean optimization (maximize average reward, ignore variance)
  # λ=0.1-0.3: Balanced (prefer consistent learning with slight reward trade-off)
  # λ=0.5+: Conservative (strongly penalize inconsistent seeds)
  # Recommended starting value: 0.2 (moderate penalty for learning fragility)
  stability_penalty_weight: 0.2

search_space:
  # Learning rate: Step size for policy updates
  # Log scale recommended (orders of magnitude matter more than linear differences)
  learning_rate:
    type: float
    low: 1.0e-5
    high: 1.0e-3
    log: true
  
  # Rollout length: Number of steps per rollout buffer collection
  # Trades off sample efficiency (low) vs variance reduction (high)
  n_steps:
    type: int
    low: 128
    high: 2048
    step: 128
  
  # Discount factor: Weight on future rewards (0=myopic, 1=far-sighted)
  # Close to 1.0 for long-term planning tasks
  gamma:
    type: float
    low: 0.9
    high: 0.999
  
  # GAE lambda: Exponential averaging parameter for advantage estimation
  # Higher values reduce bias but increase variance
  gae_lambda:
    type: float
    low: 0.9
    high: 0.99
  
  # Entropy coefficient: Bonus for policy entropy (encourages exploration)
  # Higher values = more exploration; 0.0 = no exploration bonus
  ent_coef:
    type: categorical
    choices: [0.0, 0.001, 0.01, 0.1]
  
  # Clip range: Maximum policy change per update (PPO-specific)
  # Lower values = more conservative updates
  clip_range:
    type: float
    low: 0.1
    high: 0.3
  
  # Batch size: Number of samples per gradient update
  # Must divide n_steps evenly for efficiency
  batch_size:
    type: categorical
    choices: [32, 64, 128, 256]
  
  # Number of gradient update epochs per rollout
  # Higher values = more thorough learning but risk overfitting
  n_epochs:
    type: int
    low: 3
    high: 10
    step: 1
