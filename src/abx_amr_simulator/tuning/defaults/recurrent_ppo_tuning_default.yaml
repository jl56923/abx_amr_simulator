# Default RecurrentPPO hyperparameter tuning configuration.
#
# This tuning config defines the search space and optimization settings for
# RecurrentPPO agents (LSTM-based policy for POMDP environments). Use with tune.py
# to find optimal hyperparameters for recurrent architectures.
#
# Compatible algorithms: RecurrentPPO
#
# Example usage:
#   python -m abx_amr_simulator.training.tune \
#     --umbrella-config /path/to/base_experiment.yaml \
#     --tuning-config /path/to/recurrent_ppo_tuning_default.yaml \
#     --run-name rppo_tuning_baseline
#
# Key differences from standard PPO tuning:
# - LSTM-based policy requires more conservative learning rates for training stability
# - Shorter rollout lengths (n_steps) for effective LSTM memory management
# - Maintains 4:1 ratio vs feedforward PPO (recurrent uses ~1/4 rollout length)
# - Designed for POMDP settings: delayed observations, noisy AMR, partial visibility
#
# When to use this config:
# - Any scenario with delayed/noisy AMR observations and recurrent agents
# - Partial observability or history-dependent decision-making

optimization:
  # Number of trials to run (each trial tests one hyperparameter combination)
  # Default reduced to 40 for faster baseline exploration
  n_trials: 40
  
  # Number of random seeds per trial (results aggregated with mean-variance penalty)
  # Higher values increase robustness but increase runtime
  n_seeds_per_trial: 3
  
  # Reduced episode count for fast tuning trials
  # RecurrentPPO collects ~512 env-steps per rollout (vs 2,048 for flat PPO)
  # 50 episodes × 512 steps ≈ 25k env-steps, sufficient for learning signal
  truncated_episodes: 50
  
  # Optimization objective ('maximize' for reward, 'minimize' for loss)
  direction: maximize
  
  # Sampler algorithm: 'TPE' (Tree-structured Parzen Estimator), 'Random', or 'CMAES'
  # TPE is recommended for most cases (adaptive Bayesian optimization)
  sampler: TPE
  
  # Stability penalty weight λ for mean-variance objective: mean(rewards) - λ * std(rewards)
  # λ=0.0: Pure mean optimization (maximize average reward, ignore variance)
  # λ=0.1-0.3: Balanced (prefer consistent learning with slight reward trade-off)
  # λ=0.5+: Conservative (strongly penalize inconsistent seeds)
  # RecurrentPPO uses same penalty as flat PPO (0.2) since both are flat architectures
  stability_penalty_weight: 0.2
  
  # TPE sampler configuration (optional; all values can be auto-inferred)
  # Settings control parallel Bayesian optimization behavior
  tpe_config:
    # Number of random trials before TPE sampler begins adaptive learning
    # null = auto-infer as min(n_workers, max(5, n_trials // 4))
    # Recommended: leave as null unless you have specific tuning needs
    # Overridable via CLI: --n-startup-trials
    n_startup_trials: null
    
    # Use constant_liar strategy for parallel worker suggestions
    # 'auto' = enable if parallelism ratio (n_workers / n_trials) > 0.15
    # 'on' = always enable (beneficial for high parallelism)
    # 'off' = never enable (use only if workers are very slow)
    # Recommended: leave as 'auto' for adaptive behavior
    # Overridable via CLI: --constant-liar
    constant_liar: auto

search_space:
  # Learning rate: Step size for policy updates
  # Log scale recommended; recurrent networks benefit from more conservative rates
  # Upper bound reduced from 1e-3 to 5e-4 for LSTM training stability
  learning_rate:
    type: float
    low: 1.0e-5
    high: 5.0e-4
    log: true
  
  # Rollout length: Number of environment steps per rollout buffer collection
  # CRITICAL: RecurrentPPO requires shorter sequences for effective LSTM memory
  #
  # Scaling rationale (feedforward:recurrent = 4:1):
  # - Flat PPO: 128-2048 steps (baseline)
  # - RecurrentPPO: 64-512 steps (1/4 of flat PPO range)
  #
  # This maintains:
  # - Comparable sample efficiency across architectures
  # - Manageable sequence length for LSTM training
  # - Consistent training dynamics (update frequency, gradient flow)
  n_steps:
    type: int
    low: 64
    high: 512
    step: 64
  
  # Discount factor: Weight on future rewards (0=myopic, 1=far-sighted)
  # Close to 1.0 for long-term planning tasks
  gamma:
    type: float
    low: 0.9
    high: 0.999
  
  # GAE lambda: Exponential averaging parameter for advantage estimation
  # Higher values reduce bias but increase variance
  gae_lambda:
    type: float
    low: 0.9
    high: 0.99
  
  # Entropy coefficient: Bonus for policy entropy (encourages exploration)
  # Higher values = more exploration; 0.0 = no exploration bonus
  ent_coef:
    type: categorical
    choices: [0.01, 0.05, 0.1, 0.2, 0.3]
  
  # Clip range: Maximum policy change per update (PPO-specific)
  # Lower values = more conservative updates
  clip_range:
    type: float
    low: 0.1
    high: 0.3
  
  # Number of gradient update epochs per rollout
  # Higher values = more thorough learning but risk overfitting
  n_epochs:
    type: int
    low: 3
    high: 10
    step: 1
