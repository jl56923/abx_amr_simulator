# Default HRL-PPO hyperparameter tuning configuration.
#
# This tuning config defines the search space and optimization settings for
# hierarchical RL agents using PPO at the manager level. Use with tune.py to
# find optimal hyperparameters for the manager policy.
#
# Compatible algorithms: HRL_PPO, HRL_RPPO
#
# Example usage:
#   python -m abx_amr_simulator.training.tune \
#     --umbrella-config /path/to/hrl_experiment.yaml \
#     --tuning-config /path/to/hrl_ppo_tuning_default.yaml \
#     --run-name hrl_tuning_baseline
#
# Key differences from standard PPO tuning:
# - Manager operates in option space (discrete action: select option)
# - Temporal abstraction changes effective discount factor
# - Observation space includes front-edge patient features and AMR levels
# - Learning signal may be sparser (macro-actions take multiple steps)
#
# Search space covers manager-specific hyperparameters:
# - learning_rate: May benefit from lower values (sparser feedback)
# - n_steps: Effective rollout length in option-steps (not env-steps)
# - gamma: Discount factor for macro-rewards (aggregated over option duration)
# - option_gamma: Discount factor within option execution (used by OptionsWrapper)

optimization:
  # Number of trials to run (each trial tests one hyperparameter combination)
  n_trials: 50
  
  # Number of random seeds per trial (results aggregated with mean-variance penalty)
  # Higher values increase robustness but increase runtime
  n_seeds_per_trial: 3
  
  # Reduced episode count for fast tuning trials
  # HRL may need slightly longer trials to show meaningful learning
  truncated_episodes: 75
  
  # Optimization objective ('maximize' for reward, 'minimize' for loss)
  direction: maximize
  
  # Sampler algorithm: 'TPE' (Tree-structured Parzen Estimator), 'Random', or 'CMAES'
  # TPE is recommended for most cases (adaptive Bayesian optimization)
  sampler: TPE
  
  # Stability penalty weight λ for mean-variance objective: mean(rewards) - λ * std(rewards)
  # λ=0.0: Pure mean optimization (maximize average reward, ignore variance)
  # λ=0.1-0.3: Balanced (prefer consistent learning with slight reward trade-off)
  # λ=0.5+: Conservative (strongly penalize inconsistent seeds)
  # HRL default slightly higher than PPO (0.3 vs 0.2) to account for option discovery noise
  stability_penalty_weight: 0.3

search_space:
  # Learning rate: Step size for manager policy updates
  # Log scale recommended; may benefit from conservative values due to sparser signal
  learning_rate:
    type: float
    low: 5.0e-6
    high: 5.0e-4
    log: true
  
  # Rollout length: Number of macro-actions (option selections) per rollout
  # Note: This is in option-steps, not environment steps
  # Lower values may be appropriate (fewer macro-decisions per episode)
  n_steps:
    type: int
    low: 64
    high: 1024
    step: 64
  
  # Manager gamma: Discount factor for macro-rewards (option-level)
  # Typically high (manager makes strategic long-term choices)
  gamma:
    type: float
    low: 0.95
    high: 0.999
  
  # GAE lambda: Exponential averaging parameter for advantage estimation
  # Higher values may be beneficial (fewer but more important decisions)
  gae_lambda:
    type: float
    low: 0.9
    high: 0.99
  
  # Entropy coefficient: Bonus for manager policy entropy (encourages option diversity)
  # May want higher values to explore different option sequences
  ent_coef:
    type: categorical
    choices: [0.0, 0.01, 0.05, 0.1]
  
  # Clip range: Maximum policy change per update (PPO-specific)
  # Conservative values appropriate for hierarchical policies
  clip_range:
    type: float
    low: 0.1
    high: 0.3
  
  # Batch size: Number of macro-action samples per gradient update
  # Should match manager's effective rollout length
  batch_size:
    type: categorical
    choices: [16, 32, 64, 128]
  
  # Number of gradient update epochs per rollout
  # HRL may benefit from more thorough updates (fewer samples per rollout)
  n_epochs:
    type: int
    low: 5
    high: 15
    step: 1
  
  # Option gamma: Discount factor used within option execution by OptionsWrapper
  # Controls how future rewards within an option are weighted
  # Typically high (options span multiple steps)
  option_gamma:
    type: float
    low: 0.95
    high: 0.999
