# HRL RecurrentPPO agent configuration
# Manager policy uses RecurrentPPO over option indices.

algorithm: "HRL_RPPO"

# LSTM-specific architecture parameters
lstm_kwargs:
  lstm_hidden_size: 64  # Hidden state dimension for LSTM layers
  n_lstm_layers: 1      # Number of stacked LSTM layers
  enable_critic_lstm: true  # Whether critic network also uses LSTM

# Policy network architecture (pre-LSTM feature extraction)
policy_kwargs:
  net_arch:
    - 128
    - 128

# RecurrentPPO hyperparameters for the manager policy
recurrent_ppo:
  learning_rate: 3.0e-4
  n_steps: 256
  batch_size: 64
  n_epochs: 10
  gamma: 0.99
  gae_lambda: 0.95
  clip_range: 0.2
  ent_coef: 0.02
  vf_coef: 0.5
  max_grad_norm: 0.5
  verbose: 0
